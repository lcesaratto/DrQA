{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import spacy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokens(object):\n",
    "    \"\"\"A class to represent a list of tokenized text.\"\"\"\n",
    "    TEXT = 0\n",
    "    TEXT_WS = 1\n",
    "    SPAN = 2\n",
    "    POS = 3\n",
    "    LEMMA = 4\n",
    "    NER = 5\n",
    "\n",
    "    def __init__(self, data, annotators, opts=None):\n",
    "        self.data = data\n",
    "        self.annotators = annotators\n",
    "        self.opts = opts or {}\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"The number of tokens.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def slice(self, i=None, j=None):\n",
    "        \"\"\"Return a view of the list of tokens from [i, j).\"\"\"\n",
    "        new_tokens = copy.copy(self)\n",
    "        new_tokens.data = self.data[i: j]\n",
    "        return new_tokens\n",
    "\n",
    "    def untokenize(self):\n",
    "        \"\"\"Returns the original text (with whitespace reinserted).\"\"\"\n",
    "        return ''.join([t[self.TEXT_WS] for t in self.data]).strip()\n",
    "\n",
    "    def words(self, uncased=False):\n",
    "        \"\"\"Returns a list of the text of each token\n",
    "\n",
    "        Args:\n",
    "            uncased: lower cases text\n",
    "        \"\"\"\n",
    "        if uncased:\n",
    "            return [t[self.TEXT].lower() for t in self.data]\n",
    "        else:\n",
    "            return [t[self.TEXT] for t in self.data]\n",
    "\n",
    "    def offsets(self):\n",
    "        \"\"\"Returns a list of [start, end) character offsets of each token.\"\"\"\n",
    "        return [t[self.SPAN] for t in self.data]\n",
    "\n",
    "    def pos(self):\n",
    "        \"\"\"Returns a list of part-of-speech tags of each token.\n",
    "        Returns None if this annotation was not included.\n",
    "        \"\"\"\n",
    "        if 'pos' not in self.annotators:\n",
    "            return None\n",
    "        return [t[self.POS] for t in self.data]\n",
    "\n",
    "    def lemmas(self):\n",
    "        \"\"\"Returns a list of the lemmatized text of each token.\n",
    "        Returns None if this annotation was not included.\n",
    "        \"\"\"\n",
    "        if 'lemma' not in self.annotators:\n",
    "            return None\n",
    "        return [t[self.LEMMA] for t in self.data]\n",
    "\n",
    "    def entities(self):\n",
    "        \"\"\"Returns a list of named-entity-recognition tags of each token.\n",
    "        Returns None if this annotation was not included.\n",
    "        \"\"\"\n",
    "        if 'ner' not in self.annotators:\n",
    "            return None\n",
    "        return [t[self.NER] for t in self.data]\n",
    "\n",
    "    def ngrams(self, n=1, uncased=False, filter_fn=None, as_strings=True):\n",
    "        \"\"\"Returns a list of all ngrams from length 1 to n.\n",
    "\n",
    "        Args:\n",
    "            n: upper limit of ngram length\n",
    "            uncased: lower cases text\n",
    "            filter_fn: user function that takes in an ngram list and returns\n",
    "              True or False to keep or not keep the ngram\n",
    "            as_string: return the ngram as a string vs list\n",
    "        \"\"\"\n",
    "        def _skip(gram):\n",
    "            if not filter_fn:\n",
    "                return False\n",
    "            return filter_fn(gram)\n",
    "\n",
    "        words = self.words(uncased)\n",
    "        ngrams = [(s, e + 1)\n",
    "                  for s in range(len(words))\n",
    "                  for e in range(s, min(s + n, len(words)))\n",
    "                  if not _skip(words[s:e + 1])]\n",
    "\n",
    "        # Concatenate into strings\n",
    "        if as_strings:\n",
    "            ngrams = ['{}'.format(' '.join(words[s:e])) for (s, e) in ngrams]\n",
    "\n",
    "        return ngrams\n",
    "\n",
    "    def entity_groups(self):\n",
    "        \"\"\"Group consecutive entity tokens with the same NER tag.\"\"\"\n",
    "        entities = self.entities()\n",
    "        if not entities:\n",
    "            return None\n",
    "        non_ent = self.opts.get('non_ent', 'O')\n",
    "        groups = []\n",
    "        idx = 0\n",
    "        while idx < len(entities):\n",
    "            ner_tag = entities[idx]\n",
    "            # Check for entity tag\n",
    "            if ner_tag != non_ent:\n",
    "                # Chomp the sequence\n",
    "                start = idx\n",
    "                while (idx < len(entities) and entities[idx] == ner_tag):\n",
    "                    idx += 1\n",
    "                groups.append((self.slice(start, idx).untokenize(), ner_tag))\n",
    "            else:\n",
    "                idx += 1\n",
    "        return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        clean_text = text.replace('\\n', ' ')\n",
    "\n",
    "        tokens = self.nlp(clean_text)\n",
    "\n",
    "        data = []\n",
    "        for i in range(len(tokens)):\n",
    "            # Get whitespace\n",
    "            start_ws = tokens[i].idx\n",
    "            if i + 1 < len(tokens):\n",
    "                end_ws = tokens[i + 1].idx\n",
    "            else:\n",
    "                end_ws = tokens[i].idx + len(tokens[i].text)\n",
    "\n",
    "            data.append((\n",
    "                tokens[i].text,\n",
    "                text[start_ws: end_ws],\n",
    "                (tokens[i].idx, tokens[i].idx + len(tokens[i].text)),\n",
    "                tokens[i].tag_,\n",
    "                tokens[i].lemma_,\n",
    "                tokens[i].ent_type_,\n",
    "            ))\n",
    "\n",
    "        return Tokens(data=data, annotators=('lemma', 'pos', 'ner'), opts={'non_ent': ''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor():\n",
    "    def __init__(self):\n",
    "        self.tok = Tokenizer()\n",
    "\n",
    "    def _load_dataset(self, path):\n",
    "        \"\"\"Load json file and store fields separately.\"\"\"\n",
    "        with open(path) as f:\n",
    "            data = json.load(f)['data']\n",
    "        output = {'qids': [], 'questions': [], 'answers': [],\n",
    "                'contexts': [], 'qid2cid': []}\n",
    "        for article in data:\n",
    "            for paragraph in article['paragraphs']:\n",
    "                output['contexts'].append(paragraph['context'])\n",
    "                for qa in paragraph['qas']:\n",
    "                    output['qids'].append(qa['id'])\n",
    "                    output['questions'].append(qa['question'])\n",
    "                    output['qid2cid'].append(len(output['contexts']) - 1)\n",
    "                    if 'answers' in qa:\n",
    "                        output['answers'].append(qa['answers'])\n",
    "        return output\n",
    "    \n",
    "    def _tokenize(self, text):\n",
    "        tokens = self.tok.tokenize(text)\n",
    "        output = {\n",
    "            'words': tokens.words(),\n",
    "            'offsets': tokens.offsets(),\n",
    "            'pos': tokens.pos(),\n",
    "            'lemma': tokens.lemmas(),\n",
    "            'ner': tokens.entities(),\n",
    "        }\n",
    "        return output\n",
    "    \n",
    "    def _find_answer(self, offsets, begin_offset, end_offset):\n",
    "        \"\"\"Match token offsets with the char begin/end offsets of the answer.\"\"\"\n",
    "        start = [i for i, tok in enumerate(offsets) if tok[0] == begin_offset]\n",
    "        end = [i for i, tok in enumerate(offsets) if tok[1] == end_offset]\n",
    "        assert(len(start) <= 1)\n",
    "        assert(len(end) <= 1)\n",
    "        if len(start) == 1 and len(end) == 1:\n",
    "            return start[0], end[0]\n",
    "    \n",
    "    def _process_dataset(self, data):\n",
    "        q_tokens = []\n",
    "        for question in data['questions']:\n",
    "            q_tokens.append(self._tokenize(question))\n",
    "\n",
    "        c_tokens = []\n",
    "        for context in data['contexts']:\n",
    "            c_tokens.append(self._tokenize(context))\n",
    "\n",
    "        for idx in range(len(data['qids'])):\n",
    "            question = q_tokens[idx]['words']\n",
    "            qlemma = q_tokens[idx]['lemma']\n",
    "            document = c_tokens[data['qid2cid'][idx]]['words']\n",
    "            offsets = c_tokens[data['qid2cid'][idx]]['offsets']\n",
    "            lemma = c_tokens[data['qid2cid'][idx]]['lemma']\n",
    "            pos = c_tokens[data['qid2cid'][idx]]['pos']\n",
    "            ner = c_tokens[data['qid2cid'][idx]]['ner']\n",
    "            ans_tokens = []\n",
    "            if len(data['answers']) > 0:\n",
    "                for ans in data['answers'][idx]:\n",
    "                    found = self._find_answer(offsets,\n",
    "                                        ans['answer_start'],\n",
    "                                        ans['answer_start'] + len(ans['text']))\n",
    "                    if found:\n",
    "                        ans_tokens.append(found)\n",
    "            yield {\n",
    "                'id': data['qids'][idx],\n",
    "                'question': question,\n",
    "                'document': document,\n",
    "                'offsets': offsets,\n",
    "                'answers': ans_tokens,\n",
    "                'qlemma': qlemma,\n",
    "                'lemma': lemma,\n",
    "                'pos': pos,\n",
    "                'ner': ner,\n",
    "            }\n",
    "\n",
    "    def __call__(self, in_file, out_file, show_inputs=False, show_outputs=False, save=True):\n",
    "        dataset = self._load_dataset(in_file)\n",
    "        if show_inputs:\n",
    "            print(\"Input (only first example):\")\n",
    "            for key in dataset.keys():\n",
    "                print(f\"{key} : {str(dataset[key][0])[:100]}\")\n",
    "        \n",
    "        if save:\n",
    "            with open(out_file, 'w') as f:\n",
    "                for ex in self._process_dataset(dataset):\n",
    "                    f.write(json.dumps(ex) + '\\n')\n",
    "\n",
    "        if show_outputs:\n",
    "            print(\"\\nOutput (only first example):\")\n",
    "            with open(out_file, 'r') as f:\n",
    "                for key, value in json.loads(f.readline()).items():\n",
    "                    print(f\"{key} : {str(value)[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing SQuAD train dataset for DrQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (only first example):\n",
      "qids : 5733be284776f41900661182\n",
      "questions : To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "answers : [{'answer_start': 515, 'text': 'Saint Bernadette Soubirous'}]\n",
      "contexts : Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden\n",
      "qid2cid : 0\n",
      "\n",
      "Output (only first example):\n",
      "id : 5733be284776f41900661182\n",
      "question : ['To', 'whom', 'did', 'the', 'Virgin', 'Mary', 'allegedly', 'appear', 'in', '1858', 'in', 'Lourdes',\n",
      "document : ['Architecturally', ',', 'the', 'school', 'has', 'a', 'Catholic', 'character', '.', 'Atop', 'the', '\n",
      "offsets : [[0, 15], [15, 16], [17, 20], [21, 27], [28, 31], [32, 33], [34, 42], [43, 52], [52, 53], [54, 58], \n",
      "answers : [[102, 104]]\n",
      "qlemma : ['to', 'whom', 'do', 'the', 'Virgin', 'Mary', 'allegedly', 'appear', 'in', '1858', 'in', 'Lourdes', \n",
      "lemma : ['architecturally', ',', 'the', 'school', 'have', 'a', 'catholic', 'character', '.', 'atop', 'the', \n",
      "pos : ['RB', ',', 'DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', '.', 'IN', 'DT', 'NNP', 'NNP', 'POS', 'NN', 'NN', '\n",
      "ner : ['ORG', '', '', '', '', '', 'NORP', '', '', 'ORG', 'ORG', 'ORG', 'ORG', 'ORG', '', '', '', '', '', '\n"
     ]
    }
   ],
   "source": [
    "in_file = \"data/datasets/SQuAD-v1.1-train.json\"\n",
    "out_file = \"data/datasets/SQuAD-v1.1-train-processed-spacy.txt\"\n",
    "\n",
    "preprocessor = Preprocessor()\n",
    "\n",
    "preprocessor(in_file, out_file, show_inputs=True, show_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing SQuAD dev dataset for DrQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = \"data/datasets/SQuAD-v1.1-dev.json\"\n",
    "out_file = \"data/datasets/SQuAD-v1.1-dev-processed-spacy.txt\"\n",
    "\n",
    "preprocessor = Preprocessor()\n",
    "\n",
    "preprocessor(in_file, out_file)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "93b4b5566743436c947035f528e38c44cc6a06c2ee96d5f0af65aef54b1da49d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
