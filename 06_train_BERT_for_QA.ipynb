{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import    BertForQuestionAnswering,\\\n",
    "                            BertTokenizer,\\\n",
    "                            get_scheduler\n",
    "from torch.optim import AdamW\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self, model_name= \"bert-base-uncased\", batch_size=128):\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.raw_data = load_dataset('squad')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "        self.special_tokens_dict = {'additional_special_tokens': [\"[ANS_START]\",\"[ANS_END]\"]}\n",
    "        self.tokenizer.add_special_tokens(self.special_tokens_dict)\n",
    "\n",
    "    def examine_raw_dataset(self):\n",
    "        print(\"Dataset structure:\")\n",
    "        print(self.raw_data)\n",
    "        print(\"First training example:\")\n",
    "        print(self.raw_data[\"train\"][0])\n",
    "    \n",
    "    def _add_end_idx(self, answers, contexts):\n",
    "        new_answers = []\n",
    "\n",
    "        for answer, context in zip(answers, contexts):\n",
    "            # quick reformating to remove lists\n",
    "            answer['text'] = answer['text'][0]\n",
    "            answer['answer_start'] = answer['answer_start'][0]\n",
    "            # gold_text refers to the answer we are expecting to find in context\n",
    "            gold_text = answer['text']\n",
    "            # we already know the start index\n",
    "            start_idx = answer['answer_start']\n",
    "            # and ideally this would be the end index...\n",
    "            end_idx = start_idx + len(gold_text)\n",
    "\n",
    "            # ...however, sometimes squad answers are off by a character or two\n",
    "            if context[start_idx:end_idx] == gold_text:\n",
    "                # if the answer is not off :)\n",
    "                answer['answer_end'] = end_idx\n",
    "            else:\n",
    "                # this means the answer is off by 1-2 tokens\n",
    "                for n in [1, 2]:\n",
    "                    if context[start_idx-n:end_idx-n] == gold_text:\n",
    "                        answer['answer_start'] = start_idx - n\n",
    "                        answer['answer_end'] = end_idx - n\n",
    "            new_answers.append(answer)\n",
    "        return new_answers\n",
    "\n",
    "    def _generate_training_lists(self, dataset):\n",
    "        contexts = dataset[\"context\"]\n",
    "        questions = dataset[\"question\"]\n",
    "        answers =  self._add_end_idx(dataset[\"answers\"], contexts)\n",
    "        return contexts, questions, answers\n",
    "    \n",
    "    def _generate_validation_lists(self, dataset):\n",
    "        contexts = dataset[\"context\"]\n",
    "        questions = dataset[\"question\"]\n",
    "        answers = [answer[\"text\"] for answer in dataset[\"answers\"]]\n",
    "        return contexts, questions, answers\n",
    "\n",
    "    def _add_answers_special_tokens(self, contexts, answers):\n",
    "        for idx, context in enumerate(contexts):\n",
    "            start_index = answers[idx][\"answer_start\"]\n",
    "            first_part = context[:start_index]\n",
    "            second_part = context[start_index:]\n",
    "            context = first_part + \"[ANS_START] \" + second_part\n",
    "\n",
    "            shift = 12\n",
    "            while True:\n",
    "                end_index = answers[idx][\"answer_end\"] + shift\n",
    "                try:\n",
    "                    if context[end_index] == \" \":\n",
    "                        first_part = context[:end_index]\n",
    "                        second_part = context[end_index:]\n",
    "                        context = context = first_part + \" [ANS_END]\" + second_part\n",
    "                        break\n",
    "                    elif context[end_index] != \" \":\n",
    "                        shift += 1\n",
    "                except IndexError:\n",
    "                    context += \" [ANS_END]\"\n",
    "                    break\n",
    "\n",
    "            contexts[idx] = context\n",
    "    \n",
    "    def _get_answer_tokens(self, dataset, ground_truths=None):\n",
    "        start_positions, end_positions = [], []\n",
    "        counter = 0\n",
    "\n",
    "        device = torch.device(\"cuda\")\n",
    "        dataset['input_ids'] = dataset['input_ids'].to(device)\n",
    "        dataset['token_type_ids'] = dataset['token_type_ids'].to(device)\n",
    "        dataset['attention_mask'] = dataset['attention_mask'].to(device)\n",
    "        zero_tensor = torch.zeros(1).to(device)\n",
    "\n",
    "        for i in tqdm(range(len(dataset['input_ids']))):\n",
    "\n",
    "            doc_i = i - counter\n",
    "\n",
    "            special_token_start_pos = torch.nonzero(dataset['input_ids'][doc_i] == torch.tensor(30522)).flatten()\n",
    "            special_token_end_pos = torch.nonzero(dataset['input_ids'][doc_i] == torch.tensor(30523)).flatten()\n",
    "\n",
    "            answer_start_pos = special_token_start_pos\n",
    "            answer_end_pos = special_token_end_pos - 2\n",
    "\n",
    "            if len(special_token_start_pos) == 0 or len(special_token_end_pos) == 0:\n",
    "                counter += 1\n",
    "                dataset['input_ids'] = torch.cat((dataset['input_ids'][:doc_i,:],\n",
    "                                                    dataset['input_ids'][doc_i+1:,:]))\n",
    "                dataset['token_type_ids'] = torch.cat((dataset['token_type_ids'][:doc_i,:],\n",
    "                                                    dataset['token_type_ids'][doc_i+1:,:]))\n",
    "                dataset['attention_mask'] = torch.cat((dataset['attention_mask'][:doc_i,:],\n",
    "                                                    dataset['attention_mask'][doc_i+1:,:]))\n",
    "                if ground_truths is not None:\n",
    "                    ground_truths.pop(doc_i)\n",
    "            else:\n",
    "                start_positions.append(answer_start_pos)\n",
    "                end_positions.append(answer_end_pos)\n",
    "\n",
    "                dataset['input_ids'][doc_i] = torch.cat((dataset['input_ids'][doc_i][:special_token_end_pos],\n",
    "                                                            dataset['input_ids'][doc_i][special_token_end_pos+1:],\n",
    "                                                            zero_tensor))\n",
    "                dataset['input_ids'][doc_i] = torch.cat((dataset['input_ids'][doc_i][:special_token_start_pos],\n",
    "                                                            dataset['input_ids'][doc_i][special_token_start_pos+1:],\n",
    "                                                            zero_tensor))\n",
    "                dataset['token_type_ids'][doc_i] = torch.cat((dataset['token_type_ids'][doc_i][:special_token_end_pos],\n",
    "                                                            dataset['token_type_ids'][doc_i][special_token_end_pos+1:],\n",
    "                                                            zero_tensor))\n",
    "                dataset['token_type_ids'][doc_i] = torch.cat((dataset['token_type_ids'][doc_i][:special_token_start_pos],\n",
    "                                                            dataset['token_type_ids'][doc_i][special_token_start_pos+1:],\n",
    "                                                            zero_tensor))\n",
    "                dataset['attention_mask'][doc_i] = torch.cat((dataset['attention_mask'][doc_i][:special_token_end_pos],\n",
    "                                                            dataset['attention_mask'][doc_i][special_token_end_pos+1:],\n",
    "                                                            zero_tensor))\n",
    "                dataset['attention_mask'][doc_i] = torch.cat((dataset['attention_mask'][doc_i][:special_token_start_pos],\n",
    "                                                            dataset['attention_mask'][doc_i][special_token_start_pos+1:],\n",
    "                                                            zero_tensor))\n",
    "        \n",
    "        device = torch.device(\"cpu\")\n",
    "        dataset['input_ids'] = dataset['input_ids'].to(device)\n",
    "        dataset['token_type_ids'] = dataset['token_type_ids'].to(device)\n",
    "        dataset['attention_mask'] = dataset['attention_mask'].to(device)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "                \n",
    "        return start_positions, end_positions\n",
    "\n",
    "    def _create_tensor_dataset(self, dataset):\n",
    "        if set([\"start_positions\",\"end_positions\"]).issubset(dataset.keys()):\n",
    "            return TensorDataset(dataset[\"input_ids\"], dataset[\"token_type_ids\"],\n",
    "                                dataset[\"attention_mask\"], dataset[\"start_positions\"],\n",
    "                                dataset[\"end_positions\"])\n",
    "        else:\n",
    "            return TensorDataset(dataset[\"input_ids\"], dataset[\"token_type_ids\"],\n",
    "                                dataset[\"attention_mask\"])\n",
    "\n",
    "    def create_train_dataloader(self, show_info=False):\n",
    "        contexts_train, questions_train, answers_train = self._generate_training_lists(self.raw_data[\"train\"])\n",
    "\n",
    "        self._add_answers_special_tokens(contexts_train, answers_train)\n",
    "\n",
    "        train_dataset = self.tokenizer(contexts_train, questions_train,\n",
    "                                        truncation=True, padding='max_length',\n",
    "                                        max_length=512, return_tensors='pt')\n",
    "        \n",
    "        if show_info:\n",
    "            print(\"Training dataset key after tokenizing:\")\n",
    "            print(train_dataset.keys())\n",
    "            print(\"First example in training dataset after tokenizing:\")\n",
    "            print(self.tokenizer.decode(train_dataset['input_ids'][0])[:855])\n",
    "\n",
    "            print(\"[ANS_START] tokenizer token ID: \", self.tokenizer(\"[ANS_START]\")[\"input_ids\"][1])\n",
    "            print(\"[ANS_END] tokenizer token ID: \", self.tokenizer(\"[ANS_END]\")[\"input_ids\"][1])\n",
    "            print(\"[PAD] tokenizer token ID: \", self.tokenizer(\"[PAD]\")[\"input_ids\"][1])\n",
    "        \n",
    "        start_positions, end_positions = self._get_answer_tokens(train_dataset)\n",
    "        train_dataset[\"start_positions\"] = torch.tensor(start_positions)\n",
    "        train_dataset[\"end_positions\"] = torch.tensor(end_positions)\n",
    "\n",
    "        if show_info:\n",
    "            print(\"Training dataset key after adding answers start and end positions:\")\n",
    "            print(train_dataset.keys())\n",
    "            print(\"Training tensor types and sizes:\")\n",
    "            for key, value in train_dataset.items():\n",
    "                print(key, type(value), value.size())\n",
    "        \n",
    "        train_dataset = self._create_tensor_dataset(train_dataset)\n",
    "\n",
    "        train_dataloader = DataLoader(train_dataset,\n",
    "                                batch_size = self.batch_size,\n",
    "                                shuffle = True)\n",
    "\n",
    "        return train_dataloader\n",
    "\n",
    "    def create_val_dataloader(self):\n",
    "\n",
    "        contexts_validation, questions_validation, validation_ground_truths = self._generate_validation_lists(self.raw_data[\"validation\"])\n",
    "\n",
    "        validation_dataset = self.tokenizer(questions_validation, contexts_validation,\n",
    "                                                truncation=True, padding=True,\n",
    "                                                max_length=512, return_tensors='pt')\n",
    "        \n",
    "        validation_dataset = self._create_tensor_dataset(validation_dataset)\n",
    "\n",
    "        validation_dataloader = DataLoader(validation_dataset,\n",
    "                                        batch_size = self.batch_size,\n",
    "                                        shuffle = False)\n",
    "\n",
    "        return validation_dataloader, validation_ground_truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", num_epochs=3, length_dataloader=0):\n",
    "        self.model = BertForQuestionAnswering.from_pretrained(\n",
    "            model_name\n",
    "        )\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\n",
    "            model_name\n",
    "        )\n",
    "        self.optimizer = AdamW(self.model.parameters(),lr = 2e-5)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_training_steps = self.num_epochs * length_dataloader\n",
    "        self.lr_scheduler = get_scheduler(\n",
    "                                    \"linear\",\n",
    "                                    optimizer=self.optimizer,\n",
    "                                    num_warmup_steps=0,\n",
    "                                    num_training_steps=self.num_training_steps\n",
    "                                    )\n",
    "        self.device = self._get_device()\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def _get_device(self, show_info=False):\n",
    "        if torch.cuda.is_available():    \n",
    "            device = torch.device(\"cuda\")\n",
    "\n",
    "            if show_info:\n",
    "                print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "                print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "\n",
    "            if show_info:\n",
    "                print('No GPU available, using the CPU instead.')\n",
    "\n",
    "        return device\n",
    "\n",
    "    def load_model_state_dict(self, path=\"models_lisandro/BERT_model_state_dict.pt\"):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "    \n",
    "    def save_model_state_dict(self, path=\"models_lisandro/BERT_model_state_dict.pt\"):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "    \n",
    "    def train(self, train_dataloader):\n",
    "        self.model.train()\n",
    "\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            self.model.zero_grad()\n",
    "            parameters = {\n",
    "                \"input_ids\" : batch[0].to(self.device),\n",
    "                \"token_type_ids\": batch[1].to(self.device),\n",
    "                \"attention_mask\" :  batch[2].to(self.device), \n",
    "                \"start_positions\" : batch[3].to(self.device),\n",
    "                \"end_positions\" : batch[4].to(self.device),\n",
    "            }\n",
    "            outputs = self.model(**parameters)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "\n",
    "            self.optimizer.step()\n",
    "            self.lr_scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        return {\"training_loss\" : total_train_loss/len(train_dataloader)}\n",
    "    \n",
    "    def _normalize_answer(self, s):\n",
    "        \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "        def remove_articles(text):\n",
    "            return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "        def white_space_fix(text):\n",
    "            return ' '.join(text.split())\n",
    "\n",
    "        def remove_punc(text):\n",
    "            exclude = set(string.punctuation)\n",
    "            return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "        def lower(text):\n",
    "            return text.lower()\n",
    "\n",
    "        return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "    def _f1_score(self, prediction, ground_truth):\n",
    "        prediction_tokens = self._normalize_answer(prediction).split()\n",
    "        ground_truth_tokens = self._normalize_answer(ground_truth).split()\n",
    "        common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "        num_same = sum(common.values())\n",
    "        if num_same == 0:\n",
    "            return 0\n",
    "        precision = 1.0 * num_same / len(prediction_tokens)\n",
    "        recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "        return f1\n",
    "\n",
    "    def _exact_match_score(self, prediction, ground_truth):\n",
    "        return (1 if self._normalize_answer(prediction) == self._normalize_answer(ground_truth) else 0)\n",
    "\n",
    "    def _metric_max_over_ground_truths(self, metric_fn, prediction, ground_truths):\n",
    "        scores_for_ground_truths = []\n",
    "        for ground_truth in ground_truths:\n",
    "            score = metric_fn(prediction, ground_truth)\n",
    "            scores_for_ground_truths.append(score)\n",
    "        return max(scores_for_ground_truths)\n",
    "\n",
    "    def _evaluate(self, ground_truths, predictions):\n",
    "        \"\"\" Official evaluation script for v1.1 of the SQuAD dataset. \"\"\"\n",
    "        f1 = exact_match = total = 0\n",
    "\n",
    "        for ground_truths_list, prediction in zip(ground_truths, predictions):\n",
    "            total += 1\n",
    "            exact_match += self._metric_max_over_ground_truths(\n",
    "                            self._exact_match_score, prediction, ground_truths_list)\n",
    "            f1 += self._metric_max_over_ground_truths(\n",
    "                        self._f1_score, prediction, ground_truths_list)\n",
    "        \n",
    "        return {'batch_exact_match': exact_match, 'batch_f1': f1, 'batch_total': total}\n",
    "    \n",
    "    def predict(self, val_dataloader, val_groundtruth):\n",
    "        self.model.eval()\n",
    "\n",
    "        f1 = exact_match = total = 0\n",
    "\n",
    "        for idx_batch, batch in tqdm(enumerate(val_dataloader)):\n",
    "            parameters = {\n",
    "                \"input_ids\" : batch[0].to(self.device),\n",
    "                \"token_type_ids\": batch[1].to(self.device),\n",
    "                \"attention_mask\" :  batch[2].to(self.device),\n",
    "            }\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**parameters)\n",
    "\n",
    "            start_scores = outputs.start_logits\n",
    "            end_scores = outputs.end_logits\n",
    "\n",
    "            #get best answer\n",
    "            answer_start = torch.argmax(start_scores, dim=-1)\n",
    "            # Get the most likely end of answer with the argmax of the score\n",
    "            answer_end = torch.argmax(end_scores, dim=-1) +1\n",
    "\n",
    "            predictions_ids = (tensor[answer_start[idx_tensor]:answer_end[idx_tensor]] \n",
    "                                for idx_tensor, tensor in enumerate(parameters[\"input_ids\"]))\n",
    "            predictions = [self.tokenizer.decode(prediction_id) for prediction_id in predictions_ids]\n",
    "\n",
    "            ground_truths = val_groundtruth[idx_batch*BATCH_SIZE:(idx_batch+1)*BATCH_SIZE]\n",
    "\n",
    "            results = self._evaluate(ground_truths, predictions)\n",
    "            f1 += results[\"batch_f1\"]\n",
    "            exact_match += results[\"batch_exact_match\"]\n",
    "            total += results[\"batch_total\"]\n",
    "        \n",
    "        return { \"val_f1\" : 100.0 * f1 / total,\n",
    "                \"val_exact_match\" : 100.0 * exact_match / total}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune BERT from scratch for QA (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (C:\\Users\\Lisandro\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "100%|██████████| 2/2 [00:00<00:00, 40.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset key after tokenizing:\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "First example in training dataset after tokenizing:\n",
      "[CLS] architecturally, the school has a catholic character. atop the main building's gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to [ANS_START] saint bernadette soubirous [ANS_END] in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP] to whom did the virgin mary allegedly appear in 1858 in lourdes france? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [\n",
      "[ANS_START] tokenizer token ID:  30522\n",
      "[ANS_END] tokenizer token ID:  30523\n",
      "[PAD] tokenizer token ID:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [02:49<00:00, 516.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset key after adding answers start and end positions:\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'])\n",
      "Training tensor types and sizes:\n",
      "input_ids <class 'torch.Tensor'> torch.Size([87589, 512])\n",
      "token_type_ids <class 'torch.Tensor'> torch.Size([87589, 512])\n",
      "attention_mask <class 'torch.Tensor'> torch.Size([87589, 512])\n",
      "start_positions <class 'torch.Tensor'> torch.Size([87589])\n",
      "end_positions <class 'torch.Tensor'> torch.Size([87589])\n"
     ]
    }
   ],
   "source": [
    "data_class = Data(MODEL_NAME, BATCH_SIZE)\n",
    "\n",
    "train_dataloader = data_class.create_train_dataloader(show_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH 100/21898:\tTraining loss(4.972761631011963)\n",
      "BATCH 200/21898:\tTraining loss(3.631061553955078)\n",
      "BATCH 300/21898:\tTraining loss(3.799131155014038)\n",
      "BATCH 400/21898:\tTraining loss(3.615633964538574)\n",
      "BATCH 500/21898:\tTraining loss(3.315737724304199)\n",
      "BATCH 600/21898:\tTraining loss(1.8218226432800293)\n",
      "BATCH 700/21898:\tTraining loss(3.2214529514312744)\n",
      "BATCH 800/21898:\tTraining loss(2.0947859287261963)\n",
      "BATCH 900/21898:\tTraining loss(2.3019957542419434)\n",
      "BATCH 1000/21898:\tTraining loss(4.450675964355469)\n",
      "BATCH 1100/21898:\tTraining loss(2.5524730682373047)\n",
      "BATCH 1200/21898:\tTraining loss(1.5645030736923218)\n",
      "BATCH 1300/21898:\tTraining loss(1.3093781471252441)\n",
      "BATCH 1400/21898:\tTraining loss(1.8533928394317627)\n",
      "BATCH 1500/21898:\tTraining loss(2.514796018600464)\n",
      "BATCH 1600/21898:\tTraining loss(1.4977751970291138)\n",
      "BATCH 1700/21898:\tTraining loss(1.7563726902008057)\n",
      "BATCH 1800/21898:\tTraining loss(2.258014678955078)\n",
      "BATCH 1900/21898:\tTraining loss(4.301990509033203)\n",
      "BATCH 2000/21898:\tTraining loss(0.8998852968215942)\n",
      "BATCH 2100/21898:\tTraining loss(1.1250962018966675)\n",
      "BATCH 2200/21898:\tTraining loss(1.6615288257598877)\n",
      "BATCH 2300/21898:\tTraining loss(1.5558326244354248)\n",
      "BATCH 2400/21898:\tTraining loss(0.809288501739502)\n",
      "BATCH 2500/21898:\tTraining loss(2.142228603363037)\n",
      "BATCH 2600/21898:\tTraining loss(0.8442269563674927)\n",
      "BATCH 2700/21898:\tTraining loss(2.298318386077881)\n",
      "BATCH 2800/21898:\tTraining loss(1.2340610027313232)\n",
      "BATCH 2900/21898:\tTraining loss(0.7344790697097778)\n",
      "BATCH 3000/21898:\tTraining loss(2.38948392868042)\n",
      "BATCH 3100/21898:\tTraining loss(1.2050039768218994)\n",
      "BATCH 3200/21898:\tTraining loss(3.003925323486328)\n",
      "BATCH 3300/21898:\tTraining loss(2.289844512939453)\n",
      "BATCH 3400/21898:\tTraining loss(0.6845104694366455)\n",
      "BATCH 3500/21898:\tTraining loss(1.8481504917144775)\n",
      "BATCH 3600/21898:\tTraining loss(1.1389877796173096)\n",
      "BATCH 3700/21898:\tTraining loss(1.3663262128829956)\n",
      "BATCH 3800/21898:\tTraining loss(0.5570699572563171)\n",
      "BATCH 3900/21898:\tTraining loss(1.4307446479797363)\n",
      "BATCH 4000/21898:\tTraining loss(1.2786937952041626)\n",
      "BATCH 4100/21898:\tTraining loss(1.7282838821411133)\n",
      "BATCH 4200/21898:\tTraining loss(1.422539234161377)\n",
      "BATCH 4300/21898:\tTraining loss(0.934830367565155)\n",
      "BATCH 4400/21898:\tTraining loss(1.4529837369918823)\n",
      "BATCH 4500/21898:\tTraining loss(1.542895793914795)\n",
      "BATCH 4600/21898:\tTraining loss(2.1995584964752197)\n",
      "BATCH 4700/21898:\tTraining loss(0.7227615714073181)\n",
      "BATCH 4800/21898:\tTraining loss(0.9272444248199463)\n",
      "BATCH 4900/21898:\tTraining loss(0.3884812295436859)\n",
      "BATCH 5000/21898:\tTraining loss(0.8086979389190674)\n",
      "BATCH 5100/21898:\tTraining loss(0.7949339747428894)\n",
      "BATCH 5200/21898:\tTraining loss(1.2899317741394043)\n",
      "BATCH 5300/21898:\tTraining loss(2.0103092193603516)\n",
      "BATCH 5400/21898:\tTraining loss(0.7595338225364685)\n",
      "BATCH 5500/21898:\tTraining loss(1.5148069858551025)\n",
      "BATCH 5600/21898:\tTraining loss(2.56074595451355)\n",
      "BATCH 5700/21898:\tTraining loss(0.1790783405303955)\n",
      "BATCH 5800/21898:\tTraining loss(1.1260144710540771)\n",
      "BATCH 5900/21898:\tTraining loss(1.3557989597320557)\n",
      "BATCH 6000/21898:\tTraining loss(0.7420886158943176)\n",
      "BATCH 6100/21898:\tTraining loss(1.3531990051269531)\n",
      "BATCH 6200/21898:\tTraining loss(0.8399777412414551)\n",
      "BATCH 6300/21898:\tTraining loss(2.3526883125305176)\n",
      "BATCH 6400/21898:\tTraining loss(0.45059818029403687)\n",
      "BATCH 6500/21898:\tTraining loss(0.6993523240089417)\n",
      "BATCH 6600/21898:\tTraining loss(0.5307490825653076)\n",
      "BATCH 6700/21898:\tTraining loss(0.8256559371948242)\n",
      "BATCH 6800/21898:\tTraining loss(0.9875206351280212)\n",
      "BATCH 6900/21898:\tTraining loss(1.325079321861267)\n",
      "BATCH 7000/21898:\tTraining loss(0.10360544919967651)\n",
      "BATCH 7100/21898:\tTraining loss(0.973064661026001)\n",
      "BATCH 7200/21898:\tTraining loss(0.3655058741569519)\n",
      "BATCH 7300/21898:\tTraining loss(0.8073245286941528)\n",
      "BATCH 7400/21898:\tTraining loss(1.221825361251831)\n",
      "BATCH 7500/21898:\tTraining loss(1.3167364597320557)\n",
      "BATCH 7600/21898:\tTraining loss(2.95166015625)\n",
      "BATCH 7700/21898:\tTraining loss(1.9467650651931763)\n",
      "BATCH 7800/21898:\tTraining loss(1.0919816493988037)\n",
      "BATCH 7900/21898:\tTraining loss(0.41053783893585205)\n",
      "BATCH 8000/21898:\tTraining loss(0.5098070502281189)\n",
      "BATCH 8100/21898:\tTraining loss(0.6011449098587036)\n",
      "BATCH 8200/21898:\tTraining loss(1.6217806339263916)\n",
      "BATCH 8300/21898:\tTraining loss(0.45235246419906616)\n",
      "BATCH 8400/21898:\tTraining loss(2.2430827617645264)\n",
      "BATCH 8500/21898:\tTraining loss(0.3960281014442444)\n",
      "BATCH 8600/21898:\tTraining loss(1.4898892641067505)\n",
      "BATCH 8700/21898:\tTraining loss(0.6620262861251831)\n",
      "BATCH 8800/21898:\tTraining loss(1.0988487005233765)\n",
      "BATCH 8900/21898:\tTraining loss(1.241384506225586)\n",
      "BATCH 9000/21898:\tTraining loss(1.0838606357574463)\n",
      "BATCH 9100/21898:\tTraining loss(0.4923989176750183)\n",
      "BATCH 9200/21898:\tTraining loss(0.9295839071273804)\n",
      "BATCH 9300/21898:\tTraining loss(2.1794655323028564)\n",
      "BATCH 9400/21898:\tTraining loss(0.18594060838222504)\n",
      "BATCH 9500/21898:\tTraining loss(1.373042106628418)\n",
      "BATCH 9600/21898:\tTraining loss(1.6682724952697754)\n",
      "BATCH 9700/21898:\tTraining loss(0.28001299500465393)\n",
      "BATCH 9800/21898:\tTraining loss(0.04287857562303543)\n",
      "BATCH 9900/21898:\tTraining loss(2.110689640045166)\n",
      "BATCH 10000/21898:\tTraining loss(2.3185172080993652)\n",
      "BATCH 10100/21898:\tTraining loss(0.682391345500946)\n",
      "BATCH 10200/21898:\tTraining loss(0.8830407857894897)\n",
      "BATCH 10300/21898:\tTraining loss(1.1028603315353394)\n",
      "BATCH 10400/21898:\tTraining loss(0.8978180885314941)\n",
      "BATCH 10500/21898:\tTraining loss(0.7540256977081299)\n",
      "BATCH 10600/21898:\tTraining loss(1.5425505638122559)\n",
      "BATCH 10700/21898:\tTraining loss(1.232661485671997)\n",
      "BATCH 10800/21898:\tTraining loss(4.187708854675293)\n",
      "BATCH 10900/21898:\tTraining loss(4.07316780090332)\n",
      "BATCH 11000/21898:\tTraining loss(0.9762005805969238)\n",
      "BATCH 11100/21898:\tTraining loss(2.180169105529785)\n",
      "BATCH 11200/21898:\tTraining loss(1.0089890956878662)\n",
      "BATCH 11300/21898:\tTraining loss(1.6387193202972412)\n",
      "BATCH 11400/21898:\tTraining loss(2.1242294311523438)\n",
      "BATCH 11500/21898:\tTraining loss(0.7201952338218689)\n",
      "BATCH 11600/21898:\tTraining loss(2.361624002456665)\n",
      "BATCH 11700/21898:\tTraining loss(1.9737393856048584)\n",
      "BATCH 11800/21898:\tTraining loss(0.7235292196273804)\n",
      "BATCH 11900/21898:\tTraining loss(1.7181254625320435)\n",
      "BATCH 12000/21898:\tTraining loss(0.944329023361206)\n",
      "BATCH 12100/21898:\tTraining loss(1.8595221042633057)\n",
      "BATCH 12200/21898:\tTraining loss(0.5595453381538391)\n",
      "BATCH 12300/21898:\tTraining loss(1.1023800373077393)\n",
      "BATCH 12400/21898:\tTraining loss(0.8903375864028931)\n",
      "BATCH 12500/21898:\tTraining loss(2.4312281608581543)\n",
      "BATCH 12600/21898:\tTraining loss(1.0149579048156738)\n",
      "BATCH 12700/21898:\tTraining loss(1.315666675567627)\n",
      "BATCH 12800/21898:\tTraining loss(1.0506463050842285)\n",
      "BATCH 12900/21898:\tTraining loss(1.0890939235687256)\n",
      "BATCH 13000/21898:\tTraining loss(0.5999981760978699)\n",
      "BATCH 13100/21898:\tTraining loss(0.43014898896217346)\n",
      "BATCH 13200/21898:\tTraining loss(0.5032310485839844)\n",
      "BATCH 13300/21898:\tTraining loss(0.37450069189071655)\n",
      "BATCH 13400/21898:\tTraining loss(2.126457691192627)\n",
      "BATCH 13500/21898:\tTraining loss(0.16323456168174744)\n",
      "BATCH 13600/21898:\tTraining loss(1.5692204236984253)\n",
      "BATCH 13700/21898:\tTraining loss(0.6204708814620972)\n",
      "BATCH 13800/21898:\tTraining loss(0.4117494821548462)\n",
      "BATCH 13900/21898:\tTraining loss(1.3567249774932861)\n",
      "BATCH 14000/21898:\tTraining loss(0.39694708585739136)\n",
      "BATCH 14100/21898:\tTraining loss(0.43025127053260803)\n",
      "BATCH 14200/21898:\tTraining loss(0.13231690227985382)\n",
      "BATCH 14300/21898:\tTraining loss(2.5029664039611816)\n",
      "BATCH 14400/21898:\tTraining loss(2.129878520965576)\n",
      "BATCH 14500/21898:\tTraining loss(0.6747307777404785)\n",
      "BATCH 14600/21898:\tTraining loss(0.9825906157493591)\n",
      "BATCH 14700/21898:\tTraining loss(1.8947824239730835)\n",
      "BATCH 14800/21898:\tTraining loss(1.3706984519958496)\n",
      "BATCH 14900/21898:\tTraining loss(0.6839112043380737)\n",
      "BATCH 15000/21898:\tTraining loss(1.5765626430511475)\n",
      "BATCH 15100/21898:\tTraining loss(1.4739123582839966)\n",
      "BATCH 15200/21898:\tTraining loss(2.1220431327819824)\n",
      "BATCH 15300/21898:\tTraining loss(1.2083947658538818)\n",
      "BATCH 15400/21898:\tTraining loss(1.1109905242919922)\n",
      "BATCH 15500/21898:\tTraining loss(0.2567157745361328)\n",
      "BATCH 15600/21898:\tTraining loss(0.6177505254745483)\n",
      "BATCH 15700/21898:\tTraining loss(1.074244499206543)\n",
      "BATCH 15800/21898:\tTraining loss(0.5630165338516235)\n",
      "BATCH 15900/21898:\tTraining loss(0.4517735540866852)\n",
      "BATCH 16000/21898:\tTraining loss(2.4067912101745605)\n",
      "BATCH 16100/21898:\tTraining loss(1.649776577949524)\n",
      "BATCH 16200/21898:\tTraining loss(0.912987470626831)\n",
      "BATCH 16300/21898:\tTraining loss(0.3867942690849304)\n",
      "BATCH 16400/21898:\tTraining loss(1.1997851133346558)\n",
      "BATCH 16500/21898:\tTraining loss(1.4100828170776367)\n",
      "BATCH 16600/21898:\tTraining loss(0.08520112186670303)\n",
      "BATCH 16700/21898:\tTraining loss(1.1782119274139404)\n",
      "BATCH 16800/21898:\tTraining loss(1.1216504573822021)\n",
      "BATCH 16900/21898:\tTraining loss(0.3749828636646271)\n",
      "BATCH 17000/21898:\tTraining loss(0.8582327365875244)\n",
      "BATCH 17100/21898:\tTraining loss(0.6538583636283875)\n",
      "BATCH 17200/21898:\tTraining loss(0.7742222547531128)\n",
      "BATCH 17300/21898:\tTraining loss(1.7938123941421509)\n",
      "BATCH 17400/21898:\tTraining loss(0.3271404206752777)\n",
      "BATCH 17500/21898:\tTraining loss(0.24422159790992737)\n",
      "BATCH 17600/21898:\tTraining loss(0.35149797797203064)\n",
      "BATCH 17700/21898:\tTraining loss(0.46609172224998474)\n",
      "BATCH 17800/21898:\tTraining loss(1.1802341938018799)\n",
      "BATCH 17900/21898:\tTraining loss(2.011805295944214)\n",
      "BATCH 18000/21898:\tTraining loss(0.2792604863643646)\n",
      "BATCH 18100/21898:\tTraining loss(0.7604848742485046)\n",
      "BATCH 18200/21898:\tTraining loss(0.6298602223396301)\n",
      "BATCH 18300/21898:\tTraining loss(1.0342985391616821)\n",
      "BATCH 18400/21898:\tTraining loss(1.9052321910858154)\n",
      "BATCH 18500/21898:\tTraining loss(0.224007710814476)\n",
      "BATCH 18600/21898:\tTraining loss(0.24393723905086517)\n",
      "BATCH 18700/21898:\tTraining loss(0.7563732266426086)\n",
      "BATCH 18800/21898:\tTraining loss(0.9390814900398254)\n",
      "BATCH 18900/21898:\tTraining loss(0.598433256149292)\n",
      "BATCH 19000/21898:\tTraining loss(0.7278990149497986)\n",
      "BATCH 19100/21898:\tTraining loss(1.7444218397140503)\n",
      "BATCH 19200/21898:\tTraining loss(0.9082999229431152)\n",
      "BATCH 19300/21898:\tTraining loss(0.7185491919517517)\n",
      "BATCH 19400/21898:\tTraining loss(0.5296313762664795)\n",
      "BATCH 19500/21898:\tTraining loss(0.3864825963973999)\n",
      "BATCH 19600/21898:\tTraining loss(0.1937273144721985)\n",
      "BATCH 19700/21898:\tTraining loss(0.8464640378952026)\n",
      "BATCH 19800/21898:\tTraining loss(2.6780319213867188)\n",
      "BATCH 19900/21898:\tTraining loss(1.1344176530838013)\n",
      "BATCH 20000/21898:\tTraining loss(0.9770925641059875)\n",
      "BATCH 20100/21898:\tTraining loss(2.458024024963379)\n",
      "BATCH 20200/21898:\tTraining loss(1.2394886016845703)\n",
      "BATCH 20300/21898:\tTraining loss(0.5742358565330505)\n",
      "BATCH 20400/21898:\tTraining loss(0.6735999584197998)\n",
      "BATCH 20500/21898:\tTraining loss(0.3461339771747589)\n",
      "BATCH 20600/21898:\tTraining loss(0.8998956680297852)\n",
      "BATCH 20700/21898:\tTraining loss(2.262645721435547)\n",
      "BATCH 20800/21898:\tTraining loss(1.6441314220428467)\n",
      "BATCH 20900/21898:\tTraining loss(1.023531436920166)\n",
      "BATCH 21000/21898:\tTraining loss(1.0024683475494385)\n",
      "BATCH 21100/21898:\tTraining loss(0.44145047664642334)\n",
      "BATCH 21200/21898:\tTraining loss(1.372349739074707)\n",
      "BATCH 21300/21898:\tTraining loss(0.8328084349632263)\n",
      "BATCH 21400/21898:\tTraining loss(1.220057725906372)\n",
      "BATCH 21500/21898:\tTraining loss(0.6137964129447937)\n",
      "BATCH 21600/21898:\tTraining loss(0.5667456984519958)\n",
      "BATCH 21700/21898:\tTraining loss(0.014979226514697075)\n",
      "BATCH 21800/21898:\tTraining loss(0.9324733018875122)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1, 'training_loss': 1.3018079750173694}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_class = Model(MODEL_NAME, NUM_EPOCHS, len(train_dataloader))\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    metrics_dict = model_class.train(train_dataloader)\n",
    "\n",
    "    training_stats.append({\n",
    "                \"epoch\":epoch+1,\n",
    "                \"training_loss\": metrics_dict[\"training_loss\"]\n",
    "                })\n",
    "\n",
    "model_class.save_model_state_dict()\n",
    "\n",
    "training_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate fine-tuned BERT model (validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (C:\\Users\\Lisandro\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "100%|██████████| 2/2 [00:00<00:00, 250.07it/s]\n",
      "100%|██████████| 87599/87599 [02:57<00:00, 492.50it/s]\n"
     ]
    }
   ],
   "source": [
    "data_class = Data(MODEL_NAME, BATCH_SIZE)\n",
    "\n",
    "validation_dataloader, validation_ground_truths = data_class.create_val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lisandro\\Desktop\\Projects\\DrQA-1\\venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "83it [05:16,  3.82s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_f1': 80.06160313355913, 'val_exact_match': 69.66887417218543}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_class = Model(MODEL_NAME)\n",
    "model_class.load_model_state_dict()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "validation_stats = model_class.predict(validation_dataloader, validation_ground_truths)\n",
    "\n",
    "validation_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate pretrained BERT model for QA (from Huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (C:\\Users\\Lisandro\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "100%|██████████| 2/2 [00:00<00:00, 41.71it/s]\n"
     ]
    }
   ],
   "source": [
    "data_class = Data(MODEL_NAME, BATCH_SIZE)\n",
    "\n",
    "validation_dataloader, validation_ground_truths = data_class.create_val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "166it [17:22,  6.28s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_f1': 86.4149533129056, 'val_exact_match': 77.918637653737}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_class = Model(MODEL_NAME)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "validation_stats = model_class.predict(validation_dataloader, validation_ground_truths)\n",
    "\n",
    "validation_stats"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "93b4b5566743436c947035f528e38c44cc6a06c2ee96d5f0af65aef54b1da49d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
