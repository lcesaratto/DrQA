{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import unicodedata\n",
    "import copy\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedBRNN(nn.Module):\n",
    "    \"\"\"Stacked Bi-directional RNNs.\n",
    "\n",
    "    Differs from standard PyTorch library in that it has the option to save\n",
    "    and concat the hidden states between layers. (i.e. the output hidden size\n",
    "    for each sequence input is num_layers * hidden_size).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers,\n",
    "                 dropout_rate=0, dropout_output=False, rnn_type=nn.LSTM,\n",
    "                 concat_layers=False, padding=False):\n",
    "        super(StackedBRNN, self).__init__()\n",
    "        self.padding = padding\n",
    "        self.dropout_output = dropout_output\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_layers = num_layers\n",
    "        self.concat_layers = concat_layers\n",
    "        self.rnns = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            input_size = input_size if i == 0 else 2 * hidden_size\n",
    "            self.rnns.append(rnn_type(input_size, hidden_size,\n",
    "                                      num_layers=1,\n",
    "                                      bidirectional=True))\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        \"\"\"Encode either padded or non-padded sequences.\n",
    "\n",
    "        Can choose to either handle or ignore variable length sequences.\n",
    "        Always handle padding in eval.\n",
    "\n",
    "        Args:\n",
    "            x: batch * len * hdim\n",
    "            x_mask: batch * len (1 for padding, 0 for true)\n",
    "        Output:\n",
    "            x_encoded: batch * len * hdim_encoded\n",
    "        \"\"\"\n",
    "        if x_mask.data.sum() == 0:\n",
    "            # No padding necessary.\n",
    "            output = self._forward_unpadded(x, x_mask)\n",
    "        elif self.padding or not self.training:\n",
    "            # Pad if we care or if its during eval.\n",
    "            output = self._forward_padded(x, x_mask)\n",
    "        else:\n",
    "            # We don't care.\n",
    "            output = self._forward_unpadded(x, x_mask)\n",
    "\n",
    "        return output.contiguous()\n",
    "\n",
    "    def _forward_unpadded(self, x, x_mask):\n",
    "        \"\"\"Faster encoding that ignores any padding.\"\"\"\n",
    "        # Transpose batch and sequence dims\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        # Encode all layers\n",
    "        outputs = [x]\n",
    "        for i in range(self.num_layers):\n",
    "            rnn_input = outputs[-1]\n",
    "\n",
    "            # Apply dropout to hidden input\n",
    "            if self.dropout_rate > 0:\n",
    "                rnn_input = F.dropout(rnn_input,\n",
    "                                      p=self.dropout_rate,\n",
    "                                      training=self.training)\n",
    "            # Forward\n",
    "            rnn_output = self.rnns[i](rnn_input)[0]\n",
    "            outputs.append(rnn_output)\n",
    "\n",
    "        # Concat hidden layers\n",
    "        if self.concat_layers:\n",
    "            output = torch.cat(outputs[1:], 2)\n",
    "        else:\n",
    "            output = outputs[-1]\n",
    "\n",
    "        # Transpose back\n",
    "        output = output.transpose(0, 1)\n",
    "\n",
    "        # Dropout on output layer\n",
    "        if self.dropout_output and self.dropout_rate > 0:\n",
    "            output = F.dropout(output,\n",
    "                               p=self.dropout_rate,\n",
    "                               training=self.training)\n",
    "        return output\n",
    "\n",
    "    def _forward_padded(self, x, x_mask):\n",
    "        \"\"\"Slower (significantly), but more precise, encoding that handles\n",
    "        padding.\n",
    "        \"\"\"\n",
    "        # Compute sorted sequence lengths\n",
    "        lengths = x_mask.data.eq(0).long().sum(1).squeeze()\n",
    "        _, idx_sort = torch.sort(lengths, dim=0, descending=True)\n",
    "        _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "        lengths = list(lengths[idx_sort])\n",
    "\n",
    "        # Sort x\n",
    "        x = x.index_select(0, idx_sort)\n",
    "\n",
    "        # Transpose batch and sequence dims\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        # Pack it up\n",
    "        rnn_input = nn.utils.rnn.pack_padded_sequence(x, lengths)\n",
    "\n",
    "        # Encode all layers\n",
    "        outputs = [rnn_input]\n",
    "        for i in range(self.num_layers):\n",
    "            rnn_input = outputs[-1]\n",
    "\n",
    "            # Apply dropout to input\n",
    "            if self.dropout_rate > 0:\n",
    "                dropout_input = F.dropout(rnn_input.data,\n",
    "                                          p=self.dropout_rate,\n",
    "                                          training=self.training)\n",
    "                rnn_input = nn.utils.rnn.PackedSequence(dropout_input,\n",
    "                                                        rnn_input.batch_sizes)\n",
    "            outputs.append(self.rnns[i](rnn_input)[0])\n",
    "\n",
    "        # Unpack everything\n",
    "        for i, o in enumerate(outputs[1:], 1):\n",
    "            outputs[i] = nn.utils.rnn.pad_packed_sequence(o)[0]\n",
    "\n",
    "        # Concat hidden layers or take final\n",
    "        if self.concat_layers:\n",
    "            output = torch.cat(outputs[1:], 2)\n",
    "        else:\n",
    "            output = outputs[-1]\n",
    "\n",
    "        # Transpose and unsort\n",
    "        output = output.transpose(0, 1)\n",
    "        output = output.index_select(0, idx_unsort)\n",
    "\n",
    "        # Pad up to original batch sequence length\n",
    "        if output.size(1) != x_mask.size(1):\n",
    "            padding = torch.zeros(output.size(0),\n",
    "                                  x_mask.size(1) - output.size(1),\n",
    "                                  output.size(2)).type(output.data.type())\n",
    "            output = torch.cat([output, padding], 1)\n",
    "\n",
    "        # Dropout on output layer\n",
    "        if self.dropout_output and self.dropout_rate > 0:\n",
    "            output = F.dropout(output,\n",
    "                               p=self.dropout_rate,\n",
    "                               training=self.training)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqAttnMatch(nn.Module):\n",
    "    \"\"\"Given sequences X and Y, match sequence Y to each element in X.\n",
    "\n",
    "    * o_i = sum(alpha_j * y_j) for i in X\n",
    "    * alpha_j = softmax(y_j * x_i)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, identity=False):\n",
    "        super(SeqAttnMatch, self).__init__()\n",
    "        if not identity:\n",
    "            self.linear = nn.Linear(input_size, input_size)\n",
    "        else:\n",
    "            self.linear = None\n",
    "\n",
    "    def forward(self, x, y, y_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: batch * len1 * hdim\n",
    "            y: batch * len2 * hdim\n",
    "            y_mask: batch * len2 (1 for padding, 0 for true)\n",
    "        Output:\n",
    "            matched_seq: batch * len1 * hdim\n",
    "        \"\"\"\n",
    "        # Project vectors\n",
    "        if self.linear:\n",
    "            x_proj = self.linear(x.view(-1, x.size(2))).view(x.size())\n",
    "            x_proj = F.relu(x_proj)\n",
    "            y_proj = self.linear(y.view(-1, y.size(2))).view(y.size())\n",
    "            y_proj = F.relu(y_proj)\n",
    "        else:\n",
    "            x_proj = x\n",
    "            y_proj = y\n",
    "\n",
    "        # Compute scores\n",
    "        scores = x_proj.bmm(y_proj.transpose(2, 1))\n",
    "\n",
    "        # Mask padding\n",
    "        y_mask = y_mask.unsqueeze(1).expand(scores.size())\n",
    "        scores.data.masked_fill_(y_mask.data, -float('inf'))\n",
    "\n",
    "        # Normalize with softmax\n",
    "        alpha_flat = F.softmax(scores.view(-1, y.size(1)), dim=-1)\n",
    "        alpha = alpha_flat.view(-1, x.size(1), y.size(1))\n",
    "\n",
    "        # Take weighted average\n",
    "        matched_seq = alpha.bmm(y)\n",
    "        return matched_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilinearSeqAttn(nn.Module):\n",
    "    \"\"\"A bilinear attention layer over a sequence X w.r.t y:\n",
    "\n",
    "    * o_i = softmax(x_i'Wy) for x_i in X.\n",
    "\n",
    "    Optionally don't normalize output weights.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x_size, y_size, identity=False, normalize=True):\n",
    "        super(BilinearSeqAttn, self).__init__()\n",
    "        self.normalize = normalize\n",
    "\n",
    "        # If identity is true, we just use a dot product without transformation.\n",
    "        if not identity:\n",
    "            self.linear = nn.Linear(y_size, x_size)\n",
    "        else:\n",
    "            self.linear = None\n",
    "\n",
    "    def forward(self, x, y, x_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: batch * len * hdim1\n",
    "            y: batch * hdim2\n",
    "            x_mask: batch * len (1 for padding, 0 for true)\n",
    "        Output:\n",
    "            alpha = batch * len\n",
    "        \"\"\"\n",
    "        Wy = self.linear(y) if self.linear is not None else y\n",
    "        xWy = x.bmm(Wy.unsqueeze(2)).squeeze(2)\n",
    "        xWy.data.masked_fill_(x_mask.data, -float('inf'))\n",
    "        if self.normalize:\n",
    "            if self.training:\n",
    "                # In training we output log-softmax for NLL\n",
    "                alpha = F.log_softmax(xWy, dim=-1)\n",
    "            else:\n",
    "                # ...Otherwise 0-1 probabilities\n",
    "                alpha = F.softmax(xWy, dim=-1)\n",
    "        else:\n",
    "            alpha = xWy.exp()\n",
    "        return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSeqAttn(nn.Module):\n",
    "    \"\"\"Self attention over a sequence:\n",
    "\n",
    "    * o_i = softmax(Wx_i) for x_i in X.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearSeqAttn, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: batch * len * hdim\n",
    "            x_mask: batch * len (1 for padding, 0 for true)\n",
    "        Output:\n",
    "            alpha: batch * len\n",
    "        \"\"\"\n",
    "        x_flat = x.view(-1, x.size(-1))\n",
    "        scores = self.linear(x_flat).view(x.size(0), x.size(1))\n",
    "        scores.data.masked_fill_(x_mask.data, -float('inf'))\n",
    "        alpha = F.softmax(scores, dim=-1)\n",
    "        return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnDocReader(nn.Module):\n",
    "    def __init__(self, vocab_size, num_features, embedding_dim=300, normalize=True):\n",
    "        super(RnnDocReader, self).__init__()\n",
    "        # Word embeddings (+1 for padding)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      padding_idx=0)\n",
    "\n",
    "        # Projection for attention weighted question\n",
    "        self.qemb_match = SeqAttnMatch(embedding_dim)\n",
    "\n",
    "        # Input size to RNN: word emb + question emb + manual features\n",
    "        doc_input_size = embedding_dim * 2+ num_features\n",
    "\n",
    "        # RNN document encoder\n",
    "        self.doc_rnn = StackedBRNN(\n",
    "            input_size=doc_input_size,\n",
    "            hidden_size=128,\n",
    "            num_layers=3, # Number of encoding layers for document\n",
    "            dropout_rate=0.4,\n",
    "            dropout_output=True,\n",
    "            concat_layers=True,\n",
    "            rnn_type=nn.LSTM,\n",
    "            padding=False, # Explicitly account for padding in RNN encoding\n",
    "        )\n",
    "\n",
    "        # RNN question encoder\n",
    "        self.question_rnn = StackedBRNN(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=128,\n",
    "            num_layers=3,\n",
    "            dropout_rate=0.4,\n",
    "            dropout_output=True,\n",
    "            concat_layers=True,\n",
    "            rnn_type=nn.LSTM,\n",
    "            padding=False,\n",
    "        )\n",
    "\n",
    "        # Output sizes of rnn encoders\n",
    "        doc_hidden_size = 2 * 128 # 2 layers, 128 neurons\n",
    "        question_hidden_size = 2 * 128\n",
    "        # if concatenate rnn layers:\n",
    "        doc_hidden_size *= 3\n",
    "        question_hidden_size *= 3\n",
    "\n",
    "        # Question merging\n",
    "        self.self_attn = LinearSeqAttn(question_hidden_size)\n",
    "\n",
    "        # Bilinear attention for span start/end\n",
    "        self.start_attn = BilinearSeqAttn(\n",
    "            doc_hidden_size,\n",
    "            question_hidden_size,\n",
    "            normalize=normalize,\n",
    "        )\n",
    "        self.end_attn = BilinearSeqAttn(\n",
    "            doc_hidden_size,\n",
    "            question_hidden_size,\n",
    "            normalize=normalize,\n",
    "        )\n",
    "    \n",
    "    def _weighted_avg(self, x, weights):\n",
    "        \"\"\"Return a weighted average of x (a sequence of vectors).\n",
    "\n",
    "        Args:\n",
    "            x: batch * len * hdim\n",
    "            weights: batch * len, sum(dim = 1) = 1\n",
    "        Output:\n",
    "            x_avg: batch * hdim\n",
    "        \"\"\"\n",
    "        return weights.unsqueeze(1).bmm(x).squeeze(1)\n",
    "\n",
    "    def forward(self, x1, x1_f, x1_mask, x2, x2_mask, dropout_emb=0.3):\n",
    "        \"\"\"Inputs:\n",
    "        x1 = document word indices             [batch * len_d]\n",
    "        x1_f = document word features indices  [batch * len_d * nfeat]\n",
    "        x1_mask = document padding mask        [batch * len_d]\n",
    "        x2 = question word indices             [batch * len_q]\n",
    "        x2_mask = question padding mask        [batch * len_q]\n",
    "        \"\"\"\n",
    "        # Embed both document and question\n",
    "        x1_emb = self.embedding(x1)\n",
    "        x2_emb = self.embedding(x2)\n",
    "\n",
    "        # Dropout on embeddings\n",
    "        if dropout_emb > 0:\n",
    "            x1_emb = nn.functional.dropout(x1_emb, p=dropout_emb,\n",
    "                                           training=self.training)\n",
    "            x2_emb = nn.functional.dropout(x2_emb, p=dropout_emb,\n",
    "                                           training=self.training)\n",
    "\n",
    "        # Form document encoding inputs\n",
    "        drnn_input = [x1_emb]\n",
    "\n",
    "        # Add attention-weighted question representation\n",
    "        x2_weighted_emb = self.qemb_match(x1_emb, x2_emb, x2_mask)\n",
    "        drnn_input.append(x2_weighted_emb)\n",
    "\n",
    "        # Add manual features\n",
    "        drnn_input.append(x1_f)\n",
    "\n",
    "        # Encode document with RNN\n",
    "        doc_hiddens = self.doc_rnn(torch.cat(drnn_input, 2), x1_mask)\n",
    "\n",
    "        # Encode question with RNN + merge hiddens\n",
    "        question_hiddens = self.question_rnn(x2_emb, x2_mask)\n",
    "        q_merge_weights = self.self_attn(question_hiddens, x2_mask)\n",
    "        question_hidden = self._weighted_avg(question_hiddens, q_merge_weights)\n",
    "\n",
    "        # Predict start and end positions\n",
    "        start_scores = self.start_attn(doc_hiddens, question_hidden, x1_mask)\n",
    "        end_scores = self.end_attn(doc_hiddens, question_hidden, x1_mask)\n",
    "        return start_scores, end_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    NULL = '<NULL>'\n",
    "    UNK = '<UNK>'\n",
    "    START = 2\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize(token):\n",
    "        return unicodedata.normalize('NFD', token)\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tok2ind = {self.NULL: 0, self.UNK: 1}\n",
    "        self.ind2tok = {0: self.NULL, 1: self.UNK}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tok2ind)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.tok2ind)\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        if type(key) == int:\n",
    "            return key in self.ind2tok\n",
    "        elif type(key) == str:\n",
    "            return self.normalize(key) in self.tok2ind\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if type(key) == int:\n",
    "            return self.ind2tok.get(key, self.UNK)\n",
    "        if type(key) == str:\n",
    "            return self.tok2ind.get(self.normalize(key),\n",
    "                                    self.tok2ind.get(self.UNK))\n",
    "\n",
    "    def __setitem__(self, key, item):\n",
    "        if type(key) == int and type(item) == str:\n",
    "            self.ind2tok[key] = item\n",
    "        elif type(key) == str and type(item) == int:\n",
    "            self.tok2ind[key] = item\n",
    "        else:\n",
    "            raise RuntimeError('Invalid (key, item) types.')\n",
    "\n",
    "    def add(self, token):\n",
    "        token = self.normalize(token)\n",
    "        if token not in self.tok2ind:\n",
    "            index = len(self.tok2ind)\n",
    "            self.tok2ind[token] = index\n",
    "            self.ind2tok[index] = token\n",
    "\n",
    "    def tokens(self):\n",
    "        \"\"\"Get dictionary tokens.\n",
    "\n",
    "        Return all the words indexed by this dictionary, except for special\n",
    "        tokens.\n",
    "        \"\"\"\n",
    "        tokens = [k for k in self.tok2ind.keys()\n",
    "                  if k not in {'<NULL>', '<UNK>'}]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokens(object):\n",
    "    \"\"\"A class to represent a list of tokenized text.\"\"\"\n",
    "    TEXT = 0\n",
    "    TEXT_WS = 1\n",
    "    SPAN = 2\n",
    "    POS = 3\n",
    "    LEMMA = 4\n",
    "    NER = 5\n",
    "\n",
    "    def __init__(self, data, annotators, opts=None):\n",
    "        self.data = data\n",
    "        self.annotators = annotators\n",
    "        self.opts = opts or {}\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"The number of tokens.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def slice(self, i=None, j=None):\n",
    "        \"\"\"Return a view of the list of tokens from [i, j).\"\"\"\n",
    "        new_tokens = copy.copy(self)\n",
    "        new_tokens.data = self.data[i: j]\n",
    "        return new_tokens\n",
    "\n",
    "    def untokenize(self):\n",
    "        \"\"\"Returns the original text (with whitespace reinserted).\"\"\"\n",
    "        return ''.join([t[self.TEXT_WS] for t in self.data]).strip()\n",
    "\n",
    "    def words(self, uncased=False):\n",
    "        \"\"\"Returns a list of the text of each token\n",
    "\n",
    "        Args:\n",
    "            uncased: lower cases text\n",
    "        \"\"\"\n",
    "        if uncased:\n",
    "            return [t[self.TEXT].lower() for t in self.data]\n",
    "        else:\n",
    "            return [t[self.TEXT] for t in self.data]\n",
    "\n",
    "    def offsets(self):\n",
    "        \"\"\"Returns a list of [start, end) character offsets of each token.\"\"\"\n",
    "        return [t[self.SPAN] for t in self.data]\n",
    "\n",
    "    def pos(self):\n",
    "        \"\"\"Returns a list of part-of-speech tags of each token.\n",
    "        Returns None if this annotation was not included.\n",
    "        \"\"\"\n",
    "        if 'pos' not in self.annotators:\n",
    "            return None\n",
    "        return [t[self.POS] for t in self.data]\n",
    "\n",
    "    def lemmas(self):\n",
    "        \"\"\"Returns a list of the lemmatized text of each token.\n",
    "        Returns None if this annotation was not included.\n",
    "        \"\"\"\n",
    "        if 'lemma' not in self.annotators:\n",
    "            return None\n",
    "        return [t[self.LEMMA] for t in self.data]\n",
    "\n",
    "    def entities(self):\n",
    "        \"\"\"Returns a list of named-entity-recognition tags of each token.\n",
    "        Returns None if this annotation was not included.\n",
    "        \"\"\"\n",
    "        if 'ner' not in self.annotators:\n",
    "            return None\n",
    "        return [t[self.NER] for t in self.data]\n",
    "\n",
    "    def ngrams(self, n=1, uncased=False, filter_fn=None, as_strings=True):\n",
    "        \"\"\"Returns a list of all ngrams from length 1 to n.\n",
    "\n",
    "        Args:\n",
    "            n: upper limit of ngram length\n",
    "            uncased: lower cases text\n",
    "            filter_fn: user function that takes in an ngram list and returns\n",
    "              True or False to keep or not keep the ngram\n",
    "            as_string: return the ngram as a string vs list\n",
    "        \"\"\"\n",
    "        def _skip(gram):\n",
    "            if not filter_fn:\n",
    "                return False\n",
    "            return filter_fn(gram)\n",
    "\n",
    "        words = self.words(uncased)\n",
    "        ngrams = [(s, e + 1)\n",
    "                  for s in range(len(words))\n",
    "                  for e in range(s, min(s + n, len(words)))\n",
    "                  if not _skip(words[s:e + 1])]\n",
    "\n",
    "        # Concatenate into strings\n",
    "        if as_strings:\n",
    "            ngrams = ['{}'.format(' '.join(words[s:e])) for (s, e) in ngrams]\n",
    "\n",
    "        return ngrams\n",
    "\n",
    "    def entity_groups(self):\n",
    "        \"\"\"Group consecutive entity tokens with the same NER tag.\"\"\"\n",
    "        entities = self.entities()\n",
    "        if not entities:\n",
    "            return None\n",
    "        non_ent = self.opts.get('non_ent', 'O')\n",
    "        groups = []\n",
    "        idx = 0\n",
    "        while idx < len(entities):\n",
    "            ner_tag = entities[idx]\n",
    "            # Check for entity tag\n",
    "            if ner_tag != non_ent:\n",
    "                # Chomp the sequence\n",
    "                start = idx\n",
    "                while (idx < len(entities) and entities[idx] == ner_tag):\n",
    "                    idx += 1\n",
    "                groups.append((self.slice(start, idx).untokenize(), ner_tag))\n",
    "            else:\n",
    "                idx += 1\n",
    "        return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def tokenize(self, text, return_raw_data=False):\n",
    "        clean_text = text.replace('\\n', ' ')\n",
    "\n",
    "        tokens = self.nlp(clean_text)\n",
    "\n",
    "        data = []\n",
    "        for i in range(len(tokens)):\n",
    "            # Get whitespace\n",
    "            start_ws = tokens[i].idx\n",
    "            if i + 1 < len(tokens):\n",
    "                end_ws = tokens[i + 1].idx\n",
    "            else:\n",
    "                end_ws = tokens[i].idx + len(tokens[i].text)\n",
    "\n",
    "            data.append((\n",
    "                tokens[i].text,\n",
    "                text[start_ws: end_ws],\n",
    "                (tokens[i].idx, tokens[i].idx + len(tokens[i].text)),\n",
    "                tokens[i].tag_,\n",
    "                tokens[i].lemma_,\n",
    "                tokens[i].ent_type_,\n",
    "            ))\n",
    "        \n",
    "        if return_raw_data:\n",
    "            return data\n",
    "        return Tokens(data=data, annotators=('lemma', 'pos', 'ner'), opts={'non_ent': ''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, model_file):\n",
    "        saved_params = torch.load(model_file)\n",
    "        self.word_dict = saved_params['word_dict']\n",
    "        self.feature_dict = saved_params['feature_dict']\n",
    "        self.state_dict = saved_params['state_dict']\n",
    "        \n",
    "        self.vocab_size = len(self.word_dict)\n",
    "        self.num_features = len(self.feature_dict)\n",
    "        self.model = RnnDocReader(self.vocab_size, self.num_features)\n",
    "        self.model.load_state_dict(self.state_dict)\n",
    "\n",
    "        self.device = self._get_device()\n",
    "        # Move model to gpu\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def _get_device(self, show_info = False):\n",
    "        if torch.cuda.is_available():    \n",
    "            device = torch.device(\"cuda\")\n",
    "\n",
    "            if show_info:\n",
    "                print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "                print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "\n",
    "            if show_info:\n",
    "                print('No GPU available, using the CPU instead.')\n",
    "\n",
    "        return device\n",
    "    \n",
    "    def _tokenize(self, tokenizer_class, text):\n",
    "        tokens = tokenizer_class.tokenize(text)\n",
    "        output = {\n",
    "            'words': tokens.words(),\n",
    "            'offsets': tokens.offsets(),\n",
    "            'pos': tokens.pos(),\n",
    "            'lemma': tokens.lemmas(),\n",
    "            'ner': tokens.entities(),\n",
    "        }\n",
    "        return output\n",
    "\n",
    "    def _vectorize(self,ex, word_dict, feature_dict, single_answer=False):\n",
    "        \"\"\"Torchify a single example.\"\"\"\n",
    "\n",
    "        # Index words\n",
    "        document = torch.LongTensor([word_dict[w] for w in ex['document']])\n",
    "        question = torch.LongTensor([word_dict[w] for w in ex['question']])\n",
    "\n",
    "        # Create extra features vector\n",
    "        if len(feature_dict) > 0:\n",
    "            features = torch.zeros(len(ex['document']), len(feature_dict))\n",
    "        else:\n",
    "            features = None\n",
    "\n",
    "        # f_{exact_match}\n",
    "        q_words_cased = {w for w in ex['question']}\n",
    "        q_words_uncased = {w.lower() for w in ex['question']}\n",
    "        q_lemma = {w for w in ex['qlemma']}\n",
    "        for i in range(len(ex['document'])):\n",
    "            if ex['document'][i] in q_words_cased:\n",
    "                features[i][feature_dict['in_question']] = 1.0\n",
    "            if ex['document'][i].lower() in q_words_uncased:\n",
    "                features[i][feature_dict['in_question_uncased']] = 1.0\n",
    "            if q_lemma and ex['lemma'][i] in q_lemma:\n",
    "                features[i][feature_dict['in_question_lemma']] = 1.0\n",
    "\n",
    "        # f_{token} (POS)\n",
    "        for i, w in enumerate(ex['pos']):\n",
    "            f = 'pos=%s' % w\n",
    "            if f in feature_dict:\n",
    "                features[i][feature_dict[f]] = 1.0\n",
    "\n",
    "        # f_{token} (NER)\n",
    "        for i, w in enumerate(ex['ner']):\n",
    "            f = 'ner=%s' % w\n",
    "            if f in feature_dict:\n",
    "                features[i][feature_dict[f]] = 1.0\n",
    "\n",
    "        # f_{token} (TF)\n",
    "        counter = Counter([w.lower() for w in ex['document']])\n",
    "        l = len(ex['document'])\n",
    "        for i, w in enumerate(ex['document']):\n",
    "            features[i][feature_dict['tf']] = counter[w.lower()] * 1.0 / l\n",
    "\n",
    "        # Maybe return without target\n",
    "        if 'answers' not in ex:\n",
    "            return document, features, question, ex['id']\n",
    "\n",
    "        # ...or with target(s) (might still be empty if answers is empty)\n",
    "        if single_answer:\n",
    "            assert(len(ex['answers']) > 0)\n",
    "            start = torch.LongTensor(1).fill_(ex['answers'][0][0])\n",
    "            end = torch.LongTensor(1).fill_(ex['answers'][0][1])\n",
    "        else:\n",
    "            start = [a[0] for a in ex['answers']]\n",
    "            end = [a[1] for a in ex['answers']]\n",
    "\n",
    "        return document, features, question, start, end, ex['id']\n",
    "\n",
    "    def _batchify(self, batch):    \n",
    "        \"\"\"Gather a batch of individual examples into one batch.\"\"\"\n",
    "        NUM_INPUTS = 3\n",
    "        NUM_TARGETS = 2\n",
    "        NUM_EXTRA = 1\n",
    "\n",
    "        ids = [ex[-1] for ex in batch]\n",
    "        docs = [ex[0] for ex in batch]\n",
    "        features = [ex[1] for ex in batch]\n",
    "        questions = [ex[2] for ex in batch]\n",
    "\n",
    "        \n",
    "        # Batch documents and features\n",
    "        max_length = max([d.size(0) for d in docs])\n",
    "        x1 = torch.LongTensor(len(docs), max_length).zero_()\n",
    "        x1_mask = torch.BoolTensor(len(docs), max_length).fill_(1) # ByteTensor\n",
    "        if features[0] is None:\n",
    "            x1_f = None\n",
    "        else:\n",
    "            x1_f = torch.zeros(len(docs), max_length, features[0].size(1))\n",
    "        for i, d in enumerate(docs):\n",
    "            x1[i, :d.size(0)].copy_(d)\n",
    "            x1_mask[i, :d.size(0)].fill_(0)\n",
    "            if x1_f is not None:\n",
    "                x1_f[i, :d.size(0)].copy_(features[i])\n",
    "\n",
    "        # Batch questions\n",
    "        max_length = max([q.size(0) for q in questions])\n",
    "        x2 = torch.LongTensor(len(questions), max_length).zero_()\n",
    "        x2_mask = torch.BoolTensor(len(questions), max_length).fill_(1)\n",
    "        for i, q in enumerate(questions):\n",
    "            x2[i, :q.size(0)].copy_(q)\n",
    "            x2_mask[i, :q.size(0)].fill_(0)\n",
    "\n",
    "        # Maybe return without targets\n",
    "        if len(batch[0]) == NUM_INPUTS + NUM_EXTRA:\n",
    "            return x1, x1_f, x1_mask, x2, x2_mask, ids\n",
    "\n",
    "        elif len(batch[0]) == NUM_INPUTS + NUM_EXTRA + NUM_TARGETS:\n",
    "            # ...Otherwise add targets\n",
    "            if torch.is_tensor(batch[0][3]):\n",
    "                y_s = torch.cat([ex[3] for ex in batch])\n",
    "                y_e = torch.cat([ex[4] for ex in batch])\n",
    "            else:\n",
    "                y_s = [ex[3] for ex in batch]\n",
    "                y_e = [ex[4] for ex in batch]\n",
    "        else:\n",
    "            raise RuntimeError('Incorrect number of inputs per example.')\n",
    "        \n",
    "        return x1, x1_f, x1_mask, x2, x2_mask, y_s, y_e, ids\n",
    "\n",
    "    def _decode(self, score_s, score_e, top_n=1, max_len=None):\n",
    "        \"\"\"Take argmax of constrained score_s * score_e.\n",
    "\n",
    "        Args:\n",
    "            score_s: independent start predictions\n",
    "            score_e: independent end predictions\n",
    "            top_n: number of top scored pairs to take\n",
    "            max_len: max span length to consider\n",
    "        \"\"\"\n",
    "        pred_s = []\n",
    "        pred_e = []\n",
    "        pred_score = []\n",
    "        max_len = max_len or score_s.size(1)\n",
    "        for i in range(score_s.size(0)):\n",
    "            # Outer product of scores to get full p_s * p_e matrix\n",
    "            scores = torch.ger(score_s[i], score_e[i])\n",
    "\n",
    "            # Zero out negative length and over-length span scores\n",
    "            scores.triu_().tril_(max_len - 1)\n",
    "\n",
    "            # Take argmax or top n\n",
    "            scores = scores.numpy()\n",
    "            scores_flat = scores.flatten()\n",
    "            if top_n == 1:\n",
    "                idx_sort = [np.argmax(scores_flat)]\n",
    "            elif len(scores_flat) < top_n:\n",
    "                idx_sort = np.argsort(-scores_flat)\n",
    "            else:\n",
    "                idx = np.argpartition(-scores_flat, top_n)[0:top_n]\n",
    "                idx_sort = idx[np.argsort(-scores_flat[idx])]\n",
    "            s_idx, e_idx = np.unravel_index(idx_sort, scores.shape)\n",
    "            pred_s.append(s_idx)\n",
    "            pred_e.append(e_idx)\n",
    "            pred_score.append(scores_flat[idx_sort])\n",
    "            \n",
    "        return pred_s, pred_e, pred_score\n",
    "\n",
    "    def _predict(self, ex, candidates=None, top_n=1):\n",
    "        \"\"\"Forward a batch of examples only to get predictions.\n",
    "        \"\"\"\n",
    "        # Eval mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # Transfer to GPU\n",
    "        inputs = [e if e is None else e.to(self.device) for e in ex[:5]]\n",
    "\n",
    "        # Run forward\n",
    "        with torch.no_grad():\n",
    "            score_s, score_e = self.model(*inputs)\n",
    "\n",
    "        # Decode predictions\n",
    "        score_s = score_s.data.to('cpu')\n",
    "        score_e = score_e.data.to('cpu')\n",
    "        if candidates:\n",
    "            args = (score_s, score_e, candidates, top_n, 15)\n",
    "            # return decode_candidates(*args)\n",
    "            print(\"CANDIDATES\")\n",
    "        else:\n",
    "            args = (score_s, score_e, top_n, 15)\n",
    "            return self._decode(*args)\n",
    "\n",
    "    def predict_batch(self, batch, top_n=1):\n",
    "        \"\"\"Predict a batch of document - question pairs.\"\"\"\n",
    "        documents, questions, candidates = [], [], []\n",
    "        for b in batch:\n",
    "            documents.append(b[0])\n",
    "            questions.append(b[1])\n",
    "            candidates.append(b[2] if len(b) == 3 else None)\n",
    "        candidates = candidates if any(candidates) else None\n",
    "\n",
    "        tok = Tokenizer()\n",
    "        # Tokenize the inputs, perhaps multi-processed.\n",
    "        q_tokens = []\n",
    "        for question in questions:\n",
    "            q_tokens.append(tok.tokenize(question))\n",
    "        d_tokens = []\n",
    "        for document in documents:\n",
    "            d_tokens.append(tok.tokenize(document))\n",
    "\n",
    "        examples = []\n",
    "        for i in range(len(questions)):\n",
    "            examples.append({\n",
    "                'id': i,\n",
    "                'question': q_tokens[i].words(),\n",
    "                'qlemma': q_tokens[i].lemmas(),\n",
    "                'document': d_tokens[i].words(),\n",
    "                'lemma': d_tokens[i].lemmas(),\n",
    "                'pos': d_tokens[i].pos(),\n",
    "                'ner': d_tokens[i].entities(),\n",
    "            })\n",
    "\n",
    "        # Stick document tokens in candidates for decoding\n",
    "        if candidates:\n",
    "            candidates = [{'input': d_tokens[i], 'cands': candidates[i]}\n",
    "                            for i in range(len(candidates))]\n",
    "\n",
    "        # Build the batch and run it through the model\n",
    "        batch_exs = self._batchify([self._vectorize(e, self.word_dict, self.feature_dict) for e in examples])\n",
    "        s, e, score = self._predict(batch_exs, candidates, top_n)\n",
    "\n",
    "        # Retrieve the predicted spans\n",
    "        results = []\n",
    "        for i in range(len(s)):\n",
    "            predictions = []\n",
    "            for j in range(len(s[i])):\n",
    "                span = d_tokens[i].slice(s[i][j], e[i][j] + 1).untokenize()\n",
    "                predictions.append((span, score[i][j].item()))\n",
    "            results.append(predictions)\n",
    "            \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out pretrained DrQA Document Reader with a simple query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How long do Hamsters live?\n",
      "\n",
      "Relevant documents, with their corresponding answers:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Animal testing on Syrian hamsters</td>\n",
       "      <td>Animal testing on Syrian hamsters\\n\\nSyrian ha...</td>\n",
       "      <td>up to two weeks old</td>\n",
       "      <td>0.095373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Domestication of the Syrian hamster</td>\n",
       "      <td>Domestication of the Syrian hamster\\n\\nThe dom...</td>\n",
       "      <td>proliferated from these three colonies</td>\n",
       "      <td>0.054263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Golden hamster</td>\n",
       "      <td>Golden hamster\\n\\nThe golden hamster, or Syria...</td>\n",
       "      <td>two to three years</td>\n",
       "      <td>0.286153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hamster</td>\n",
       "      <td>Hamster\\n\\nHamsters are rodents belonging to t...</td>\n",
       "      <td>three weeks</td>\n",
       "      <td>0.014161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Hamsters (album)</td>\n",
       "      <td>The Hamsters (album)\\n\\nThe Hamsters (1993) (k...</td>\n",
       "      <td>1993</td>\n",
       "      <td>0.075838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id  \\\n",
       "0    Animal testing on Syrian hamsters   \n",
       "1  Domestication of the Syrian hamster   \n",
       "2                       Golden hamster   \n",
       "3                              Hamster   \n",
       "4                 The Hamsters (album)   \n",
       "\n",
       "                                                text  \\\n",
       "0  Animal testing on Syrian hamsters\\n\\nSyrian ha...   \n",
       "1  Domestication of the Syrian hamster\\n\\nThe dom...   \n",
       "2  Golden hamster\\n\\nThe golden hamster, or Syria...   \n",
       "3  Hamster\\n\\nHamsters are rodents belonging to t...   \n",
       "4  The Hamsters (album)\\n\\nThe Hamsters (1993) (k...   \n",
       "\n",
       "                         predicted_answer     score  \n",
       "0                     up to two weeks old  0.095373  \n",
       "1  proliferated from these three colonies  0.054263  \n",
       "2                      two to three years  0.286153  \n",
       "3                             three weeks  0.014161  \n",
       "4                                    1993  0.075838  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best prediction : two to three years, with score : 0.2861531972885132\n"
     ]
    }
   ],
   "source": [
    "MODEL_FILE = 'models_lisandro/document_reader.mdl'\n",
    "\n",
    "model_class = Model(MODEL_FILE)\n",
    "\n",
    "\n",
    "query = \"How long do Hamsters live?\"\n",
    "\n",
    "selected_ids = str((\"Animal testing on Syrian hamsters\", \"Domestication of the Syrian hamster\",\n",
    "                \"Golden hamster\", \"Hamster\", \"The Hamsters (album)\"))\n",
    "\n",
    "db_path = \"data/wikipedia/docs.db\"\n",
    "connection = sqlite3.connect(db_path, check_same_thread=False)\n",
    "cursor = connection.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT id, text FROM documents WHERE id IN \" + selected_ids)\n",
    "data_json = {\"id\": [], \"text\": [], \"predicted_answer\" : [], \"score\": []}\n",
    "for r in cursor.fetchall():\n",
    "    data_json[\"id\"].append(r[0]); data_json[\"text\"].append(r[1])\n",
    "\n",
    "cursor.close()\n",
    "connection.close()\n",
    "\n",
    "examples = [[data_json['text'][idx], query] for idx in range(len(data_json[\"text\"]))]\n",
    "\n",
    "predictions = model_class.predict_batch(examples)\n",
    "\n",
    "for p in predictions:\n",
    "    data_json[\"predicted_answer\"].append(p[0][0])\n",
    "    data_json[\"score\"].append(p[0][1])\n",
    "\n",
    "data_df = pd.DataFrame.from_dict(data_json)\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(\"\\nRelevant documents, with their corresponding answers:\")\n",
    "display(data_df.head(5))\n",
    "\n",
    "# Argmax among all documents\n",
    "answers = [p[0][0] for p in predictions]\n",
    "scores = [p[0][1] for p in predictions]\n",
    "idx = torch.argmax(torch.tensor(scores))\n",
    "\n",
    "print(f\"\\nBest prediction : {answers[idx]}, with score : {scores[idx]}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "93b4b5566743436c947035f528e38c44cc6a06c2ee96d5f0af65aef54b1da49d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
